{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sc\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from math import cos, sin, atan\n",
    "\n",
    "from matplotlib.backends.backend_qt5agg import FigureCanvasQTAgg as FigureCanvas\n",
    "from matplotlib.figure import Figure\n",
    "\n",
    "import time\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Red configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedConfig():\n",
    "    def __init__(self, learning_ratio = 0.01, iteration_limit = 1000, err_max = 0.1, graphic_resolution = 50, interval_show = 2):\n",
    "        self.__learning_ratio = learning_ratio\n",
    "        self.__iteration_limit = iteration_limit\n",
    "        self.__err_max = err_max\n",
    "        self.__graphic_resolution = graphic_resolution\n",
    "        self.__interval_show = interval_show\n",
    "        \n",
    "    def get_learning_ratio(self):\n",
    "        return self.__learning_ratio\n",
    "    \n",
    "    def get_iteration_limit(self):\n",
    "        return self.__iteration_limit\n",
    "    \n",
    "    def get_err_max(self):\n",
    "        return self.__err_max\n",
    "    \n",
    "    def get_graphic_resolution(self):\n",
    "        return self.__graphic_resolution\n",
    "    \n",
    "    def get_interval_show(self):\n",
    "        return self.__interval_show\n",
    "    \n",
    "    \n",
    "    \n",
    "    def set_learning_ratio(self, _learning_ratio):\n",
    "        try:\n",
    "            self.__learning_ratio = _learning_ratio\n",
    "        except:\n",
    "            return False\n",
    "        return True\n",
    "    \n",
    "    def set_iteration_limit(self, _iteration_limit):\n",
    "        try:\n",
    "            self.__iteration_limit = _iteration_limit\n",
    "        except:\n",
    "            return False\n",
    "        return True\n",
    "        \n",
    "    def set_err_max(self, _err_max):\n",
    "        try:\n",
    "            self.__err_max = _err_max\n",
    "        except:\n",
    "            return False\n",
    "        return True\n",
    "    \n",
    "    def set_graphic_resolution(self, _graphic_resolution):\n",
    "        try:\n",
    "            self.__graphic_resolution = _graphic_resolution\n",
    "        except:\n",
    "            return False\n",
    "        return True\n",
    "    \n",
    "    def set_interval_show(self, _interval_show):\n",
    "        try:\n",
    "            self.__interval_show = _interval_show\n",
    "        except:\n",
    "            return False\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Canvas for plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Canvas(FigureCanvas):\n",
    "\n",
    "    def __init__(self, parent=None):\n",
    "        fig = Figure()\n",
    "        self.axes = fig.add_subplot(111)\n",
    "\n",
    "        self.compute_initial_figure()\n",
    "\n",
    "        FigureCanvas.__init__(self, fig)\n",
    "        self.setParent(parent)\n",
    "\n",
    "        FigureCanvas.setSizePolicy(self,\n",
    "                                   QSizePolicy.Expanding,\n",
    "                                   QSizePolicy.Expanding)\n",
    "        FigureCanvas.updateGeometry(self)\n",
    "\n",
    "    def compute_initial_figure(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network: rustic graphic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphicNeuron():\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def draw(self, neuron_radius):\n",
    "        circle = plt.Circle((self.x, self.y), radius=neuron_radius, fill=False)\n",
    "        plt.gca().add_patch(circle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphicLayer():\n",
    "    def __init__(self, network, number_of_neurons, number_of_neurons_in_widest_layer):\n",
    "        self.vertical_distance_between_layers = 6\n",
    "        self.horizontal_distance_between_neurons = 2\n",
    "        self.neuron_radius = 0.5\n",
    "        self.number_of_neurons_in_widest_layer = number_of_neurons_in_widest_layer\n",
    "        self.previous_layer = self.__get_previous_layer(network)\n",
    "        self.y = self.__calculate_layer_y_position()\n",
    "        self.neurons = self.__intialise_neurons(number_of_neurons)\n",
    "\n",
    "    def __intialise_neurons(self, number_of_neurons):\n",
    "        neurons = []\n",
    "        x = self.__calculate_left_margin_so_layer_is_centered(number_of_neurons)\n",
    "        for iteration in range(number_of_neurons):\n",
    "            neuron = GraphicNeuron(x, self.y)\n",
    "            neurons.append(neuron)\n",
    "            x += self.horizontal_distance_between_neurons\n",
    "        return neurons\n",
    "\n",
    "    def __calculate_left_margin_so_layer_is_centered(self, number_of_neurons):\n",
    "        return self.horizontal_distance_between_neurons * (self.number_of_neurons_in_widest_layer - number_of_neurons) / 2\n",
    "\n",
    "    def __calculate_layer_y_position(self):\n",
    "        if self.previous_layer:\n",
    "            return self.previous_layer.y + self.vertical_distance_between_layers\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def __get_previous_layer(self, network):\n",
    "        if len(network.layers) > 0:\n",
    "            return network.layers[-1]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def __line_between_two_neurons(self, neuron1, neuron2):\n",
    "        angle = atan((neuron2.x - neuron1.x) / float(neuron2.y - neuron1.y))\n",
    "        x_adjustment = self.neuron_radius * sin(angle)\n",
    "        y_adjustment = self.neuron_radius * cos(angle)\n",
    "        line = plt.Line2D((neuron1.x - x_adjustment, neuron2.x + x_adjustment), (neuron1.y - y_adjustment, neuron2.y + y_adjustment))\n",
    "        plt.gca().add_line(line)\n",
    "\n",
    "    def draw(self, layerType=0):\n",
    "        for neuron in self.neurons:\n",
    "            neuron.draw( self.neuron_radius )\n",
    "            if self.previous_layer:\n",
    "                for previous_layer_neuron in self.previous_layer.neurons:\n",
    "                    self.__line_between_two_neurons(neuron, previous_layer_neuron)\n",
    "        # write Text\n",
    "        x_text = self.number_of_neurons_in_widest_layer * self.horizontal_distance_between_neurons\n",
    "        if layerType == 0:\n",
    "            plt.text(x_text, self.y, 'Capa de Entrada', fontsize = 12)\n",
    "        elif layerType == -1:\n",
    "            plt.text(x_text, self.y, 'Capa de Salida ', fontsize = 12)\n",
    "        else:\n",
    "            plt.text(x_text, self.y, 'Capa Oculta '+str(layerType), fontsize = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphicNeuralNetwork():\n",
    "    def __init__(self, number_of_neurons_in_widest_layer):\n",
    "        self.number_of_neurons_in_widest_layer = number_of_neurons_in_widest_layer\n",
    "        self.layers = []\n",
    "        self.layertype = 0\n",
    "\n",
    "    def add_layer(self, number_of_neurons ):\n",
    "        layer = GraphicLayer(self, number_of_neurons, self.number_of_neurons_in_widest_layer)\n",
    "        self.layers.append(layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    def __init__(self, n_XY):\n",
    "        self.n_X = n_XY[0]\n",
    "        self.n_Y = n_XY[1]\n",
    "        self.X = []\n",
    "        self.Y = []\n",
    "        \n",
    "        \n",
    "    def import_X_dataset(self, file_manager):\n",
    "        cols = []\n",
    "        \n",
    "        for i_x in range(0, self.n_X):\n",
    "            cols.append(i_x)\n",
    "        \n",
    "        self.X = file_manager.import_dataset(cols)\n",
    "        \n",
    "    def import_Y_dataset(self, file_manager):\n",
    "        cols = []\n",
    "        \n",
    "        for i_y in range(self.n_X, self.n_X + self.n_Y):\n",
    "            cols.append(i_y)\n",
    "            \n",
    "        self.Y = file_manager.import_dataset(cols)\n",
    "        self.fix_Y_axis()\n",
    "        \n",
    "    def fix_Y_axis(self):\n",
    "        self.Y = self.Y[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Structure: neural layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural layer builder\n",
    "\n",
    "class NeuralLayer():\n",
    "    # The class is initialized receiving the parameters:\n",
    "    # n_conn: connections number, neurons of layer before\n",
    "    # n_neur: neurons number\n",
    "    # act_f: activation function\n",
    "    \n",
    "    def __init__ (self, n_connection, n_neuron, activation_function):\n",
    "        \n",
    "        self.activation_function = activation_function\n",
    "        self.W = np.random.rand(n_connection, n_neuron) * 2 - 1 \n",
    "        self.b = np.random.rand(1, n_neuron) * 2 - 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationFunction():\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.names_activation_functions_hidden = [\"Escalon\", \"Sigmoide\", \"RELU\", \"TanH\"]\n",
    "        self.names_activation_functions_final = [\"Escalon\", \"Sigmoide\", \"RELU\", \"TanH\", \"Lineal\"]\n",
    "        \n",
    "        self.step = lambda x: x >= 0\n",
    "\n",
    "        self.sigm = (lambda x: 1 / (1 + np.exp(-x)), \n",
    "                     lambda x: (lambda value=self.sigm[0](x): value * (1 - value))()\n",
    "                    )\n",
    "\n",
    "        self.relu = (lambda x: np.maximum(0, x),\n",
    "                     lambda x: x > 0\n",
    "                    )\n",
    "\n",
    "        self.tanh = (lambda x: np.tanh(x),\n",
    "                     lambda x: 1.0 - (np.tanh(x) ** 2)\n",
    "                    )\n",
    "        \n",
    "        self.linear = (lambda x: x,\n",
    "                       lambda x: 1\n",
    "                      )\n",
    "        \n",
    "#         self.gauss = (lambda x: ,\n",
    "#                      lambda x: \n",
    "#                     )\n",
    "\n",
    "    def get_array_functions(self):\n",
    "        \n",
    "        return [self.step, self.sigm, self.relu, self.tanh, self.linear]\n",
    "    \n",
    "\n",
    "\n",
    "#active = ActivationFunction()\n",
    "#_x = np.linspace(-5,5,100)   # Variable que vaya de 5 a -5 y genera 100 valores\n",
    "#plt.plot(_x,active.sigm[1](_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network: Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    \n",
    "    def __init__(self, topology, activation_functions, red_config, dataset):\n",
    "        \n",
    "        self.topology = topology\n",
    "        self.activation_functions = activation_functions\n",
    "        \n",
    "        self.dataset = dataset\n",
    "        self.red_config = red_config\n",
    "        \n",
    "        self.cost_function = (lambda Yp, Yr: np.mean((Yp - Yr) ** 2),\n",
    "                              lambda Yp, Yr: (Yp - Yr)\n",
    "                             )\n",
    "        self.create_nn()\n",
    "\n",
    "    \n",
    "    def create_nn(self):\n",
    "\n",
    "        # Vector containing the layers that make up the network\n",
    "        self.neural_network = []\n",
    "        # Review the neural network topology and create the layers\n",
    "        for l, layer in enumerate(self.topology[:-1]):\n",
    "\n",
    "            self.neural_network.append(NeuralLayer(self.topology[l], self.topology[l+1], self.activation_functions[l+1]))\n",
    "            \n",
    "    \n",
    "    def solve_unknow_dataset(self, X):\n",
    "        \n",
    "        out = [(None, X)]\n",
    "        \n",
    "        for l, layer in enumerate(self.neural_network):\n",
    "            \n",
    "            self.feedforward(l, out)\n",
    "            \n",
    "        return out[-1][1]\n",
    "    \n",
    "    \n",
    "    def train(self, backpropagation, backpropagation_type):\n",
    "        \n",
    "        out = [(None, self.dataset.X)]\n",
    "        \n",
    "        for l, layer in enumerate(self.neural_network):\n",
    "            \n",
    "            self.feedforward(l, out)\n",
    "            \n",
    "        if backpropagation:\n",
    "\n",
    "            self.backward(out, backpropagation_type)\n",
    "\n",
    "        else:\n",
    "\n",
    "            self.normal_Wb_update(out)\n",
    "            \n",
    "        return out[-1][1]\n",
    "        \n",
    "    \n",
    "    def feedforward(self, l, out):\n",
    "        \n",
    "        # Weighted sum of layer[l]\n",
    "        z = out[-1][1] @ self.neural_network[l].W + self.neural_network[l].b\n",
    "\n",
    "        # Activation value of layer[l]\n",
    "        a = self.neural_network[l].activation_function[0](z)\n",
    "        \n",
    "        # Save both values of layer[l] to vector\n",
    "        out.append((z,a))\n",
    "        \n",
    "        \n",
    "    def backward(self, out, backpropagation_type):\n",
    "        \n",
    "        δ = []\n",
    "        \n",
    "        for l in reversed(range(0, len(self.neural_network))):\n",
    "            \n",
    "            z = out[l+1][0]\n",
    "            a = out[l+1][1]\n",
    "            \n",
    "            if l == len(self.neural_network)-1 :\n",
    "                \n",
    "                δ.insert(0, self.cost_function[1](a, self.dataset.Y) * self.neural_network[l].activation_function[1](z))\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                δ.insert(0, δ[0] @ _W.T * self.neural_network[l].activation_function[1](z))\n",
    "                \n",
    "            _W = self.neural_network[l].W\n",
    "            \n",
    "            #GRADIENT DESCENT\n",
    "            \n",
    "            ##Primitive = 0 ; Cascade = 1\n",
    "            if backpropagation_type:\n",
    "                self.neural_network[l].b = self.neural_network[l].b - np.mean(δ[0], axis=0, keepdims=True) * self.red_config.get_learning_ratio()\n",
    "                self.neural_network[l].W = self.neural_network[l].W - out[l][1].T @ δ[0] * self.red_config.get_learning_ratio()\n",
    "            else:\n",
    "                self.neural_network[l].b = self.neural_network[l].b + np.mean(δ[0], axis=0, keepdims=True) * self.red_config.get_learning_ratio()\n",
    "                self.neural_network[l].W = self.neural_network[l].W + out[l][1].T @ δ[0] * self.red_config.get_learning_ratio()\n",
    "            \n",
    "    def normal_Wb_update(self, out):\n",
    "        \n",
    "        δ = []\n",
    "        \n",
    "        for l in reversed(range(0, len(self.neural_network))):\n",
    "            \n",
    "            z = out[l+1][0]\n",
    "            a = out[l+1][1]\n",
    "            \n",
    "            if l == len(self.neural_network)-1 :\n",
    "                \n",
    "                δ.insert(0, self.cost_function[1](a, self.dataset.Y))\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                δ.insert(0, δ[0] @ _W.T)\n",
    "                \n",
    "            _W = self.neural_network[l].W\n",
    "            \n",
    "            #GRADIENT DESCENT\n",
    "\n",
    "            self.neural_network[l].b = self.neural_network[l].b - np.mean(δ[0], axis=0, keepdims=True) * self.red_config.get_learning_ratio()\n",
    "            self.neural_network[l].W = self.neural_network[l].W - out[l][1].T @ δ[0] * self.red_config.get_learning_ratio()\n",
    "\n",
    "#         for l in range(1, len(self.neural_network)):\n",
    "            \n",
    "#             z = out[l-1][0]\n",
    "#             a = out[l-1][1]\n",
    "            \n",
    "#             if l == len(self.neural_network)-1 :\n",
    "                \n",
    "#                 δ.append(self.cost_function[1](a, self.dataset.Y))\n",
    "                \n",
    "#             else:\n",
    "                \n",
    "#                 δ.append(δ[-1] @ _W.T)\n",
    "                \n",
    "#             _W = self.neural_network[l].W\n",
    "        \n",
    "#             self.neural_network[l].b = self.neural_network[l].b - np.mean(δ[-1], axis=0, keepdims=True) * self.red_config.get_learning_ratio()\n",
    "        \n",
    "#             self.neural_network[l].W = self.neural_network[l].W - out[l][1].T @ δ[-1] * self.red_config.get_learning_ratio()\n",
    "            \n",
    "\n",
    "#     def execute(self, backpropagation, backpropagation_type):\n",
    "        \n",
    "#         err_iterations = []\n",
    "#         x_axis = []\n",
    "        \n",
    "#         #Here need to call interval_generator()\n",
    "        \n",
    "#         for i in range(self.red_config.get_iteration_limit()):\n",
    "            \n",
    "#             Y_iteration = self.train(backpropagation, backpropagation_type)\n",
    "            \n",
    "#             if i % self.red_config.get_interval_show() == 0 :\n",
    "                \n",
    "#                 err_iterations.append(self.cost_function[0](Y_iteration, self.dataset.Y))\n",
    "                \n",
    "#                 #Need to call plot_generator()\n",
    "#                 x_axis.append(i)\n",
    "#                 plt.plot(x_axis, err_iterations)\n",
    "#                 clear_output(wait=True)\n",
    "#                 plt.show()\n",
    "#                 time.sleep(1)\n",
    "            \n",
    "#             if err_iterations[-1] <= self.red_config.get_err_max():\n",
    "                \n",
    "#                 if i % self.red_config.get_interval_show() != 0:\n",
    "                    \n",
    "#                     err_iterations.append(self.cost_function[0](Y_iteration, self.dataset.Y))\n",
    "                \n",
    "#                     #Need to call plot_generator()\n",
    "                    \n",
    "#                     clear_output(wait=True)\n",
    "#                     plt.plot(range(0, len(err_iterations),self.red_config.get_iteration_limit()), err_iterations)\n",
    "#                     plt.show()\n",
    "#                     time.sleep(1)\n",
    "                \n",
    "#                 return err_iterations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
